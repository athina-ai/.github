# Welcome to Athina AI

_Athina is building monitoring and evaluation tools for LLM developers._

![hero](https://github.com/athina-ai/.github/assets/7515552/c9da94de-8854-41ab-8f24-d4522b634c83)


## Open-Source Framework for Evals
[athina-ai/athina-evals](https://github.com/athina-ai/athina-evals)

[Documentation](https://docs.athina.ai/evals) | [Quick Start](https://docs.athina.ai/evals/quick_start) | [Running Evals](https://docs.athina.ai/evals/running_evals) 

We have a library of preset evaluators, but you can also write custom evaluators within the Athina framework. 

#### Example Preset Evals:

- **Context Contains Enough Information**: Detect bad or insufficient retrievals.
- **Does Response Answer Query**: Detect incomplete or irrelevant responses.
- **Response Faithfulness**: Detect when responses are deviating from the provided context.
- **Summarization Accuracy**: Detect hallucinations and mistakes in summaries
- **Grading Criteria**: If X, then fail. Otherwise pass.
- **Custom Evals**: Custom prompt for LLM-powered evaluation.
- **RAGAS**: A set of evaluators that return [RAGAS](https://github.com/explodinggradients/ragas) metrics.

Results can also be viewed and tracked on our platform.
![develop-view](https://github.com/athina-ai/.github/assets/7515552/5aaf9d9f-8462-45b2-81c9-946c800991fd)



## Monitoring & Evaluations Platform for LLM Inferences

[Documentation](https://docs.athina.ai/monitoring) | [Demo Video](https://bit.ly/athina-platform-demo) | [Sign Up](https://app.athina.ai)

- UI for monitoring and visibility into your LLM inferences.
- Run evals automatically against logged inferences in production.
- Track cost, token usage, response times, feedback, pass rate and other eval metrics.
- Analytics segmented by Customer ID, Model, Prompt, Environment, and More.
- Topic Classification
- Data Exports
- ... and more

Contact hello@athina.ai if you have any questions.
